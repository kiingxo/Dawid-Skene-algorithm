üóìÔ∏è Saturday 26th July
------------------------

Topic: Revisiting Introduction to Fine-Tuning
------------------------------------------------

What is Fine-Tuning?
-------------------------
Fine-tuning an LLM customizes its behavior, enhances and injects new knowledge, and optimizes performance for specific domains or tasks.


Fine-Tuning vs Pre-Training
----------------------------

Pre-Training Analogy
---------------------
Like teaching a child everything about the world ‚Äî language, math, science, history ‚Äî from scratch.

Fine-Tuning Analogy
--------------------
Like teaching that same child how to do one job really well ‚Äî e.g., become a lawyer, doctor, or customer support agent.


Common Fine-Tuning Methods
------------------------------

	‚Ä¢	SFT (Supervised Fine-Tuning) ‚Äî Task-specific labeled data
	‚Ä¢	LoRA / QLoRA ‚Äî Low-rank adaptation, lightweight fine-tuning
	‚Ä¢	PEFT ‚Äî Parameter-Efficient Fine-Tuning (only updates a small subset of model weights)
	‚Ä¢	Instruction Fine-Tuning ‚Äî Teaches models to better follow human-written instructions

When Should You Fine-Tune?
------------------------------------
	‚Ä¢	You have domain-specific data (e.g. medical, legal, financial)
	‚Ä¢	You want better performance than prompting can give
	‚Ä¢	You plan to deploy models offline
	‚Ä¢	You need custom behavior (e.g., specific tone, formatting, task alignment)

If you don‚Äôt have labeled data or just want better responses:
	‚Ä¢	Prompt engineering ‚Äî Improve your prompts, not the model
	‚Ä¢	RAG (Retrieval-Augmented Generation) ‚Äî Feed external knowledge in real time
	‚Ä¢	Embeddings + vector search ‚Äî Inject relevant context at inference time




LoRA or QLoRA
--------------

Both are techniques used to fine-tune large language models efficiently, without updating the entire model (which would be expensive and slow).


LoRA (Low-Rank Adaptation)
---------------------------
Fine-tunes just a few small matrices inside the model instead of the whole thing.


	‚Ä¢	Saves time and memory
	‚Ä¢	Uses 16-bit precision (fp16/bf16)  (still pretty fast)
	‚Ä¢	Good for small-to-medium models
	‚Ä¢	Can be run on consumer GPUs (like 12‚Äì24 GB VRAM....hard it works fine with colab too)



 QLoRA (Quantized LoRA)
 ---------------------
 Takes LoRA and adds 4-bit quantization, making the model even smaller and lighter to train.
	‚Ä¢	Great for very large models (like 33B, 70B, etc.)
	‚Ä¢	Even more memory efficient than LoRA
	‚Ä¢	Can run massive models on smaller GPUs (like 1x A100 or even consumer GPUs with ~24 GB)
	‚Ä¢	But: it‚Äôs a bit trickier to set up, especially for beginners



Will be starting with Llama 3.1 (8B)
------------------------------------
ill explain what parameters are 
	‚Ä¢	‚Äú8B‚Äù = 8 billion parameters
‚Üí A parameter is like a ‚Äúdial‚Äù in the model that adjusts how it understands language.
‚Üí More parameters = more knowledge and reasoning capacity (but also more compute/memory needed).



Parameter vs modal size

Precision        Storage per parameter       Model size for 8B params
32-bit float       4 bytes                   8B √ó 4 = ~32 GB
16-bit float	   2 bytes                   8B √ó 2 = ~16 GB
4-bit Quantized	   0.5 bytes                 8B √ó 0.5 = ~4 GB



Choosing the Right Model for Fine-Tuning
-------------------------------------------

1. Match Model to Use Case
	‚Ä¢	Vision tasks ‚Üí Use LLaMA 3.2 Vision
	‚Ä¢	Code tasks ‚Üí Use Qwen Coder 2.5
	‚Ä¢	Always align model type with your dataset/task.

2. Check Licensing & Requirements
	‚Ä¢	Ensure compatibility with your system.
	‚Ä¢	Review VRAM and compute requirements using provided guidelines.

3. Evaluate Resources
	‚Ä¢	Choose a model size that fits your VRAM, dataset size, and training budget.
	‚Ä¢	Larger models need more storage and compute.

4. Select the Model Version
	‚Ä¢	Prefer the latest model for best performance.
