üóìÔ∏è Saturday 26th July
------------------------

Topic: Revisiting Introduction to Fine-Tuning
------------------------------------------------

What is Fine-Tuning?
-------------------------
Fine-tuning an LLM customizes its behavior, enhances and injects new knowledge, and optimizes performance for specific domains or tasks.


Fine-Tuning vs Pre-Training
----------------------------

Pre-Training Analogy
---------------------
Like teaching a child everything about the world ‚Äî language, math, science, history ‚Äî from scratch.

Fine-Tuning Analogy
--------------------
Like teaching that same child how to do one job really well ‚Äî e.g., become a lawyer, doctor, or customer support agent.


Common Fine-Tuning Methods
------------------------------

	‚Ä¢	SFT (Supervised Fine-Tuning) ‚Äî Task-specific labeled data
	‚Ä¢	LoRA / QLoRA ‚Äî Low-rank adaptation, lightweight fine-tuning
	‚Ä¢	PEFT ‚Äî Parameter-Efficient Fine-Tuning (only updates a small subset of model weights)
	‚Ä¢	Instruction Fine-Tuning ‚Äî Teaches models to better follow human-written instructions

When Should You Fine-Tune?
------------------------------------
	‚Ä¢	You have domain-specific data (e.g. medical, legal, financial)
	‚Ä¢	You want better performance than prompting can give
	‚Ä¢	You plan to deploy models offline
	‚Ä¢	You need custom behavior (e.g., specific tone, formatting, task alignment)

If you don‚Äôt have labeled data or just want better responses:
	‚Ä¢	Prompt engineering ‚Äî Improve your prompts, not the model
	‚Ä¢	RAG (Retrieval-Augmented Generation) ‚Äî Feed external knowledge in real time
	‚Ä¢	Embeddings + vector search ‚Äî Inject relevant context at inference time




LoRA or QLoRA
--------------

Both are techniques used to fine-tune large language models efficiently, without updating the entire model (which would be expensive and slow).


LoRA (Low-Rank Adaptation)
---------------------------
Fine-tunes just a few small matrices inside the model instead of the whole thing.


	‚Ä¢	Saves time and memory
	‚Ä¢	Uses 16-bit precision (fp16/bf16)  (still pretty fast)
	‚Ä¢	Good for small-to-medium models
	‚Ä¢	Can be run on consumer GPUs (like 12‚Äì24 GB VRAM....hard it works fine with colab too)



 QLoRA (Quantized LoRA)
 ---------------------
 Takes LoRA and adds 4-bit quantization, making the model even smaller and lighter to train.
	‚Ä¢	Great for very large models (like 33B, 70B, etc.)
	‚Ä¢	Even more memory efficient than LoRA
	‚Ä¢	Can run massive models on smaller GPUs (like 1x A100 or even consumer GPUs with ~24 GB)
	‚Ä¢	But: it‚Äôs a bit trickier to set up, especially for beginners



Will be starting with Llama 3.1 (8B)
------------------------------------
ill explain what parameters are 
	‚Ä¢	‚Äú8B‚Äù = 8 billion parameters
‚Üí A parameter is like a ‚Äúdial‚Äù in the model that adjusts how it understands language.
‚Üí More parameters = more knowledge and reasoning capacity (but also more compute/memory needed).



Parameter vs modal size

Precision        Storage per parameter       Model size for 8B params
32-bit float       4 bytes                   8B √ó 4 = ~32 GB
16-bit float	   2 bytes                   8B √ó 2 = ~16 GB
4-bit Quantized	   0.5 bytes                 8B √ó 0.5 = ~4 GB



Choosing the Right Model for Fine-Tuning
-------------------------------------------

1. Match Model to Use Case
	‚Ä¢	Vision tasks ‚Üí Use LLaMA 3.2 Vision
	‚Ä¢	Code tasks ‚Üí Use Qwen Coder 2.5
	‚Ä¢	Always align model type with your dataset/task.

2. Check Licensing & Requirements
	‚Ä¢	Ensure compatibility with your system.
	‚Ä¢	Review VRAM and compute requirements using provided guidelines.

3. Evaluate Resources
	‚Ä¢	Choose a model size that fits your VRAM, dataset size, and training budget.
	‚Ä¢	Larger models need more storage and compute.

4. Select the Model Version
	‚Ä¢	Prefer the latest model for best performance.


Dataset Guide for Fine-Tuning

What is a Dataset?
	‚Ä¢	A dataset is a collection of text data used to fine-tune or pretrain LLMs.
	‚Ä¢	Text must be tokenizable to be useful ‚Äî tokens are turned into embeddings the model learns from.

Key Concepts
	‚Ä¢	Chat Template: Defines how your data is structured (chat, instructions, etc.).
	‚Ä¢	Tokenization: Converts text into tokens (subwords, words, characters) for model processing.


Dataset Design Steps
	1.	Define Purpose ‚Äì What are you training the model to do? (e.g. Q&A, summarization, roleplay)
	2.	Choose Output Style ‚Äì What should the model generate? (text, JSON, code, language)
	3.	Pick Data Source ‚Äì Use CSVs, PDFs, websites, or Hugging Face datasets (e.g. ShareGPT).
‚Üí You can also generate synthetic data, but it must be high quality.

 Common Data Formats
	‚Ä¢	Raw Text Format (for CPT):
	"text": "Pasta carbonara is a traditional Roman pasta dish..."

	‚Ä¢ Instruction Format (Alpaca style):
			{
		"instruction": "Summarize this text.",
		"input": "Carbonara is made from...",
		"output": "It's a Roman pasta dish made with..."
		}

    ‚Ä¢	Conversation Format (ShareGPT style):
				{
			"conversations": [
				{"from": "human", "value": "Help me make carbonara"},
				{"from": "gpt", "value": "Do you want the traditional recipe?"}
			]
			}

 	‚Ä¢	ChatML Format (OpenAI / Hugging Face default):
			{
  "messages": [
    {"role": "user", "content": "What is 1+1?"},
    {"role": "assistant", "content": "It's 2!"}
  ]
}